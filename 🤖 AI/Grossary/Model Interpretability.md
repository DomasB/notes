**What is model interpretability (Explainable AI)?**
----------------------------------------------------

Model interpretability (also known as **explainable AI**) is the process by which a [ML](https://www.hopsworks.ai/dictionary/ml) model's predictions can be explained and understood by humans. In [MLOps](https://www.hopsworks.ai/dictionary/mlops), this typically requires logging [inference data](https://www.hopsworks.ai/dictionary/inference-data) and predictions together, so that a library (such as Alibi) or framework (such as LIME or SHAP) can later [process and produce explanations for the predictions](https://towardsdatascience.com/essential-explainable-ai-python-frameworks-that-you-should-know-about-84d5063b75e9).Â 

