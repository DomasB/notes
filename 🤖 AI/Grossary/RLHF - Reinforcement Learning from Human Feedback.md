‍**How much relative importance Is RLHF for LLMs?**

LLMs are trained on huge volumes of text, much of which contains toxic language, racism, sexism, and other problematic prose. LLMs can also hallucinate - they can invent answers that are not real, that is, they imagine an answer not based on real-world facts.

RLHF mitigates some of the problems of training on toxic data and on LLMs producing hallucinations. 

There are competing RLHF narratives about how significant the role of RLHF is for LLMs.[Yann LeCun claimed](https://twitter.com/ylecun/status/1667218790625468416) that, even with RLHF, LLMs cannot solve the problem of hallucinations - they are an inevitable by-product of the auto-regressive nature of LLMs. On the other hand, researchers, such as [Kosinski](https://arxiv.org/abs/2302.02083), showed that the LLMs, such as GPT-4, can perform close to human level on Theory-of-Mind (ToM) tasks, indicating that LLMs [can acquire beliefs and mental states](https://twitter.com/ben_levinstein/status/1678469343544328193).

