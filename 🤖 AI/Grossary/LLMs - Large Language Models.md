---
aliases:
  - LLM
  - LLMs

tags:
  - ml llm chatbot nlp ml prompt-engineering in-context-learning pre-training fine-tuning private-data huggingface llm ChatGPT paramaters memory weight open-source  
---

1. **Definition of LLMs:** LLMs (Large Language Models) are machine learning models trained on extensive text data to understand and generate human language. Notably, OpenAI's GPT-3, with 175 billion parameters, is an example of such a model.

2. **GPU Requirements:** LLMs require GPUs for both training and inference to avoid sluggish performance.

3. **Prompts and In-Context Learning:** LLMs respond to queries (prompts) in natural language. The quality of responses can be enhanced through prompt engineering, where prompts are carefully designed. In-context learning involves providing examples for the LLM to learn from.

4. **Fine Tuning:** Pre-trained LLMs can be downloaded and fine-tuned on private data for specific tasks, yielding improved performance for specialized queries.

5. **Size of LLMs (Parameters):** LLM size is measured by parameters. Larger models, like GPT-4 with 1,700 billion parameters, exhibit unique capabilities. The number of parameters also affects memory usage; for instance, Llama-2-70b with 16-bit precision requires 140 GB memory.

6. **Proprietary LLMs:** Proprietary LLMs, like OpenAI's ChatGPT and GPT-4, are accessed via UI or APIs. They can't be downloaded, and customization is limited. Legal and privacy concerns might restrict their use.

7. **Open-Source LLMs:** Numerous open-source LLMs exist. Llama-2, the most powerful, has 70 billion parameters. They can be fine-tuned for specific tasks using proprietary data. This type of LLM offers control over deployment in data centers or clouds, ensuring data privacy. However, their performance is still somewhat inferior to proprietary LLMs for general language tasks due to their smaller size.

**What are LLMs (large language models)?**
------------------------------------------

LLMs stands for Large Language Models. These are machine learning models that have been trained on massive amounts of text data, such as books, articles, and web pages, to understand and generate human language.(This definition was generated by a LLM - [OpenAI's GPT-3](https://openai.com/product) (Generative Pre-trained Transformer 3), which has 175 billion parameters and can generate highly coherent and contextually relevant language text.)

LLMs require graphic processing units (GPUs) to be trained and also for inference (otherwise they are very slow).

**Prompts and In-Context Learning**
-----------------------------------

A LLM takes a query in natural language (such as English) as input and produces a response. The input query is called a prompt. Often, you can improve the response from a LLM by carefully designing the prompt, in a process called [prompt engineering](https://www.hopsworks.ai/dictionary/prompt-engineering) or prompt tuning. LLMs can work well when you give them explicit instructions about how the output format of the response should be, or given them examples that you would like them to learn from ([in-context learning](https://www.hopsworks.ai/dictionary/in-context-learning-icl)). For example, if the LLM training cut off time was in 2021, and you provide the LLM as a prompt the wikipedia article for the 2022 football world cup, and add at the end of your prompt the query - “who won the 2022 world cup in football?”, it will answer correctly with Argentina.

‍

**Fine Tuning LLMs**
--------------------

Recently, many pre-trained LLMs have been open-sourced and can be downloaded, and then [fined-tuned](https://www.hopsworks.ai/dictionary/fine-tuning-llms) on your private data to perform a specific task for you. For example, maybe you have large amounts of documentation about your company or products. In this case, you could download a pre-trained LLM (with frozen weights, from somewhere such as [HuggingFace](https://huggingface.co/models?other=LLM)), and then add some extra layers and fine-tune those layers using your private data. You now have a model that should perform better on queries on your private data.

**Size of LLMs (number of Parameters)**
---------------------------------------

The size of a LLM is typically measured as the number of parameters it contains. The largest known model, GPT-4, has [been speculated to have 1,700 billion parameters](https://the-decoder.com/gpt-4-has-a-trillion-parameters/). In contrast, the [largest Llama-2 model has 70 billion parameters](https://huggingface.co/meta-llama/Llama-2-7b). 

The size of a LLM is important, because certain capabilities only emerge when models grow beyond certain sizes. In a paper by Wei et al from Google Research, they showed that mathematical and word skills and instruction following appear in LLMs when they grow past certain sizes and have been trained for long enough (measured in training FLOPs).

The number of parameters in a model also determines the size of the model in memory.  For example, in Llama-2, the model parameters in 16-bit precision consume:

* **Llama-2-70b with 16-bit precision = 2 bytes \* 70 billion = 140 GB of memory**

In practice, this means Llama-2-70b will [need at least 2 A100 GPUs](https://www.cursor.so/blog/llama-inference) (80GB) for inference or fine-tuning.

‍

‍

‍

‍

**Proprietary LLMs**
--------------------

The most well-known LLMs released by OpenAI (ChatGPT, GPT-4) are proprietary models - you are not able to download the models, they are only accessible via a UI on their website or via API calls. For example, you pay OpenAI to use the higher end LLMs (GPT-4) and build commercial applications that call their models via APIs. Sometimes, organizations are legally prevented from using proprietary LLMs as they are restricted on what type of data they can send to an external proprietary LLM (e.g., due to data privacy). You can customize responses from proprietary LLMs with prompt engineering, but you cannot fine-tune them.

‍

**Open-Source LLMs**
--------------------

There are now [hundreds of open-source LLMs available](https://github.com/Hannibal046/Awesome-LLM). Currently, the most powerful is Llama-2, released by Meta in July 2023, with 70 billion parameters. Open-source LLMs have the advantage over proprietary models that they can be fine-tuned for task-specific goals. Organizations may have valuable proprietary data (such as their customer help data or internal documentation) that they can leverage to build custom LLMs with fine-tuning. Open-source LLMs also enable organizations to deploy models within their own data centers or cloud accounts, so sensitive data will not leave their network. However, the largest open-source LLMs are still an order of magnitude smaller than the largest proprietary LLMs, so their performance is still not as good for general purpose language tasks.
